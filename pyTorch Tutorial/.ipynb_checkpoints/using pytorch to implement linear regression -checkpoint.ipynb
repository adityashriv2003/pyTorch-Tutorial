{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a6c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cee5a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs \n",
    "x = torch.tensor([[1] , [2] , [3] , [4]] , dtype=torch.float32)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18fa2962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "99e93d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# creating a one elt test , that should be a tensor \n",
    "x_test = torch.tensor([5] , dtype = torch.float32)\n",
    "\n",
    "# the size of the samples is 4 as the no. of data points is 4\n",
    "# the size of the feature should be one , as we are entertaining one at a time :- y = w*x (one x at a time gets multiplied and produce one o/p)\n",
    "\n",
    "n_samples , n_features = x.shape\n",
    "# features is the weight , we need to train\n",
    "print(n_features)\n",
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc6f5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input size = 1\n",
    "# output_size = 1\n",
    "input_size = n_features\n",
    "output_size = n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "97384694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training -1.4428951740264893\n"
     ]
    }
   ],
   "source": [
    "# we just need one layer , applying linear transformation to the incoming data\n",
    "model = nn.Linear(input_size , output_size)\n",
    "print(\"prediction before training\" , model(x_test).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12d46434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000002A04B58D270>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f938a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef749c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function is mean squared error\n",
    "loss = nn.MSELoss()\n",
    "# optimizer is used to update the weights according to the loss gradients \n",
    "optimizer = torch.optim.SGD(model.parameters() , lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a027441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters : - w =  1.9999780654907227 loss =  tensor(7.0747e-10, grad_fn=<MseLossBackward0>)\n",
      "parameters : - w =  1.9999786615371704 loss =  tensor(6.6611e-10, grad_fn=<MseLossBackward0>)\n",
      "parameters : - w =  1.9999792575836182 loss =  tensor(6.2751e-10, grad_fn=<MseLossBackward0>)\n",
      "parameters : - w =  1.9999799728393555 loss =  tensor(5.8878e-10, grad_fn=<MseLossBackward0>)\n",
      "parameters : - w =  1.9999804496765137 loss =  tensor(5.5105e-10, grad_fn=<MseLossBackward0>)\n",
      "parameters : - w =  1.9999810457229614 loss =  tensor(5.2400e-10, grad_fn=<MseLossBackward0>)\n",
      "parameters : - w =  1.9999815225601196 loss =  tensor(4.8919e-10, grad_fn=<MseLossBackward0>)\n",
      "parameters : - w =  1.9999821186065674 loss =  tensor(4.6448e-10, grad_fn=<MseLossBackward0>)\n",
      "parameters : - w =  1.9999827146530151 loss =  tensor(4.3521e-10, grad_fn=<MseLossBackward0>)\n",
      "parameters : - w =  1.9999830722808838 loss =  tensor(4.1094e-10, grad_fn=<MseLossBackward0>)\n",
      "prediction after training:- tensor([10.0000], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoc in range(n_iter):\n",
    "    prediction = model(x)\n",
    "    \n",
    "    l = loss(y , prediction)\n",
    "    l.backward() # calculating gradients \n",
    "    \n",
    "    optimizer.step()# updating the weights \n",
    "    \n",
    "    optimizer.zero_grad()#zeroing the calculated gradients so that they don't get added up\n",
    "    \n",
    "    if epoc % 10 == 0:\n",
    "        # weights and bias is like [[list of weights] , [list of bias]]\n",
    "        weights_bias = model.parameters()\n",
    "        weights_bias = list(weights_bias)\n",
    "        print('parameters : - w = ' , weights_bias[0][0].item() , 'loss = ' , l)\n",
    "    \n",
    "print(\"prediction after training:-\" , model(x_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
