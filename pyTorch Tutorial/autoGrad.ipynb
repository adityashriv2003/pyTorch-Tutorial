{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e66a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12c5ae9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6428, 0.2332, 0.0255], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(3, requires_grad = True)\n",
    "x\n",
    "# creates a tensor of size 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20829a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2023, 0.3429, 0.4523], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(3 , requires_grad = True)\n",
    "# if in future we need to calculate the gradient of a function w.r.t y then requires_grad = true\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03c3a70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0819, 0.2352, 0.4092], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.6428, 2.2332, 2.0255], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a mulBackward function is created to calculate the gradient of z w.r.t y\n",
    "z = y*y*2\n",
    "print(z)\n",
    "# a AddBackward function is created to calculate the gradient of k w.r.t x\n",
    "k = x+2\n",
    "k\n",
    "# computational graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbdf6de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0789, 1.8289, 2.4124])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = z.mean()\n",
    "z.backward()# dz/dy\n",
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6154ee00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8532, 0.8512, 0.8720], requires_grad=True)\n",
      "tensor([1.4558, 1.4490, 1.5208], grad_fn=<MulBackward0>)\n",
      "tensor([0.0375, 3.4387, 6.9760])\n"
     ]
    }
   ],
   "source": [
    "# right now z is just a scaler , but if z was a vector then we would need to pass a gradient vector\n",
    "m = torch.rand(3 , requires_grad = True)\n",
    "print(m)\n",
    "a = m*m*2\n",
    "print(a)\n",
    "# creating the gradient vector \n",
    "v = torch.tensor([0.011 , 1.01 , 2.0] , dtype = torch.float32)\n",
    "a.backward(v)\n",
    "print(m.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10edca99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0679, 0.3337, 0.9648], requires_grad=True)\n",
      "tensor([0.0679, 0.3337, 0.9648])\n"
     ]
    }
   ],
   "source": [
    "# how to prevent pyTorch from calculating the gradients \n",
    "# basically stop it from tracking the history in the computational graph\n",
    "# when we are updating the weights , that should not be the part of the gradient computation \n",
    "# x.requires_grad_(False)\n",
    "# whenever there is a trailing underscore after a function name that means it will alter the specified variable inplace\n",
    "#x.detach()\n",
    "# with torch.no_grad()\n",
    "\n",
    "n = torch.rand(3 , requires_grad = True)\n",
    "print(n)\n",
    "n.requires_grad_(False)\n",
    "print(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3749cde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5181, 0.9815, 0.1795], requires_grad=True)\n",
      "tensor([0.5181, 0.9815, 0.1795])\n"
     ]
    }
   ],
   "source": [
    "# METHOD 2\n",
    "h = torch.rand(3 , requires_grad = True)\n",
    "print(h)\n",
    "h_aux = h.detach()# we need to insert the value into a new a variable\n",
    "print(h_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f8c728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7882, 0.1714, 0.1881], requires_grad=True)\n",
      "tensor([2.3645, 0.5143, 0.5643])\n"
     ]
    }
   ],
   "source": [
    "# METHOD 3\n",
    "new_tensor = torch.rand(3 , requires_grad = True)\n",
    "print(new_tensor)\n",
    "with torch.no_grad():\n",
    "    # all the operations inside this block\n",
    "    # will automatically have requires_grad = False\n",
    "    new_tensor_aux = new_tensor*3\n",
    "    print(new_tensor_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4569554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.])\n",
      "tensor([6., 6., 6.])\n",
      "tensor([9., 9., 9.])\n",
      "tensor([12., 12., 12.])\n"
     ]
    }
   ],
   "source": [
    "# we need to remember that the .grad attribute will keep on adding the gradients in multiple iterations\n",
    "# so after every iteration we need to clear the gradients to zero \n",
    "\n",
    "# for eg :- \n",
    "weights = torch.ones(3 ,  requires_grad = True)\n",
    "\n",
    "for epoch in range(4):\n",
    "    model_output = (weights*3).sum()\n",
    "    # some operation on the tensor \n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c4b9f6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.])\n",
      "tensor([3., 3., 3.])\n",
      "tensor([3., 3., 3.])\n",
      "tensor([3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "# so to prevent the above problem :-\n",
    "weights1 = torch.ones(3 ,  requires_grad = True)\n",
    "\n",
    "for epoch in range(4):\n",
    "    model_output1 = (weights1*3).sum()\n",
    "    # some operation on the tensor \n",
    "    \n",
    "    model_output1.backward()\n",
    "    \n",
    "    print(weights1.grad)\n",
    "    \n",
    "    # this will reset the gradients to zero \n",
    "    weights1.grad.zero_()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c839f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# side note\n",
    "# we can also use built in optimizer\n",
    "new_weights = torch.ones(3 , requires_grad = True)\n",
    "# lr is learning rate\n",
    "#optimizer = torch.optim.SGD(new_weights ,  lr = 0.01)\n",
    "#optimizer.step()\n",
    "#optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
